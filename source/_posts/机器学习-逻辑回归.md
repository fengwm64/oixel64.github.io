---
title: '[机器学习]逻辑回归'
date: 2023-11-23 22:19:50
categories: 机器学习
tags: 
- 机器学习
- 逻辑回归
---

# 1.基本思想

逻辑回归是一种广义上的线性回归分析模型，简单来看就是将线性回归的结果y通过一个映射函数sigmoid映射到0-1区间，**将sigmoid函数输出当做概率值**，当概率值大于0.5时分类为正类，反之反类。

# 2.模型

$$\begin{aligned}&h_\theta(x)=g(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}\\&\hat{y}=\begin{cases}1,h_\theta(x)>0.5\\0,h_\theta(x)<0.5\end{cases}\end{aligned}$$

- 其中，映射函数sigmoid ：$g(z)=\frac1{1+e^{-z}}$

# 3.推导过程

## 3.1 表示正/负类概率

1. 逻辑回归是二分类算法，**定义** 正类标签为$y=1$，负类标签为$y=1$。

> 这里既然是**定义**，说明是人为规定的，原因在于这样定义对后续的推导可以起到**简化作用**，后文紧接着会提到。如果你比较叛逆，不这样定义也是可以的，就是推导会非常麻烦。

2. 接下来是第二个涉及**定义**的地方，**定义**将sigmoid函数输出为**概率值**，sigmoid函数输出是[0,1]区间的，恰好符合概率的要求。且定义**正类的概率为$h_\theta(x)$**，负类自然就是$1-h_\theta(x)$。

-------------------------

综上，

则给定一些特征$x$，其是正类的概率**写作** $P_\theta(y=1 | x)$ 或 $p$；负类的概率**写作** $P_\theta(y=0 | x)$ 或 $1-p$

关键一步的**化繁为简**，将分类为**正类的概率$p$** 和 分类为**负类的概率$1-p$** 同时用一条公式表达出来：

$$\begin{aligned}
&\text{令}\begin{cases}P_\theta(y=1| x)=h_\theta(x)\\P_\theta(y=0 | x)=1-h_\theta(x)\end{cases}\xrightarrow{\text{合并}}P_\theta(y | x)=(h_\theta(x))^y(1-h_\theta(x))^{1-y} \\
&\text{令}\begin{cases}h_\theta(x)=p\\1-h_\theta(x)=1-p\end{cases}\xrightarrow{\text{原式变换为}}P_\theta(y\mid x)=p^y\cdot(1-p)^{1-y}
\end{aligned}$$

非常非常巧妙是不是，自己尝试代入标签y=1或者y=0看看效果是不是和原先一模一样

现在，我们有了一个统一的公式表达标签及对应的概率：

$$P_\theta(y\mid x)=p^y\cdot(1-p)^{1-y}$$


## 3.2 最大似然估计

上面将sigmoid函数输出当做概率，我们目标追求正确分类的概率 $P_\theta(y\mid x)$ 要越高越好，使用最大似然估计来求使 达到最大的参数 ，故可以得到参数 的似然函数如下：

