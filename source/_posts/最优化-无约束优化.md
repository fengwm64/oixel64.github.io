---
title: '[最优化]无约束优化'
date: 2024-05-03 15:47:04
tags:
- 最优化
- 无约束优化
- 牛顿法
- 最速下降法
---

# 0. 几种优化算法关系

![](https://cdn.jsdelivr.net/gh/oixel64/imgs/imgs/202405031854745.png)

# 1. 梯度法（Gradient Descent）

## 1.1 原理

1. **目标函数的梯度**：
   
   梯度法基于目标函数的梯度信息进行迭代优化。对于目标函数 $f(x)$，梯度 $\nabla f(x)$ 表示了函数在某点 $x$ 处的变化率和变化方向。梯度的方向是函数增长最快的方向，而梯度的负方向则是函数下降最快的方向。


2. **迭代更新规则**：

   梯度法通过迭代更新当前解 $x$ 的值，使其朝着目标函数下降的方向移动。更新规则通常如下所示：
   $$ x_{k+1} = x_k - \alpha \nabla f(x_k) $$
   其中，$x_k$ 是第 $k$ 步的解，$x_{k+1}$ 是下一步的解，$\alpha$ 是学习率（也称为步长，为一个**常数**），控制了每一步迭代中移动的距离。

3. 梯度法实际上是一个**最朴素**的算法，是后续很多算法的基石


## 1.2 算法步骤

1. **初始化参数**：选择合适的初始参数值 $\mathbf{x}_0$

2. **计算梯度**：计算目标函数关于参数的梯度 $\nabla J(\mathbf{x})$。

3. **更新参数**：根据梯度的反方向调整参数值，更新参数 $\mathbf{x}$，以减小目标函数的值。更新规则为：

   $$
   \mathbf{x}_{t+1} = \mathbf{x}_t - \alpha \nabla J(\mathbf{x}_t)
   $$
   其中，$\alpha$ 是学习率（步长），控制参数更新的幅度。

4. **收敛判断**：判断是否满足停止条件，例如达到最大迭代次数、目标函数值变化小于阈值等。

5. **迭代**：若未满足停止条件，则重复步骤 2-4，直到满足停止条件为止。

## 1.3 代码实现

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.rc('text', usetex=True)

# 定义函数
def f(x):
    x_1, x_2, x_3 = x
    return (x_1 - 4)**4 + (x_2 - 3)**2 + 4*(x_3 + 5)**4

# 定义函数导数
def df_dx(x):
    x_1, x_2, x_3 = x
    df_dx1 = 4 * (x_1 - 4)**3
    df_dx2 = 2 * (x_2 - 3)
    df_dx3 = 16 * (x_3 + 5)**3
    return np.array([df_dx1, df_dx2, df_dx3])

# 初始点
x0 = np.array([4.0, 2.0, -1.0])

# 梯度下降法
def gradient_descent(f, df_dx,x0, learning_rate=0.005, epsilon=1e-6, max_iters=1000):
    x = x0
    recorder = list()
    recorder.append([0,x,0,f(x)])

    for i in range(max_iters):
        gradient = df_dx(x)
        if np.linalg.norm(gradient) < epsilon:
            break
        x = x - learning_rate * gradient
        recorder.append([i+1,x,gradient,f(x)])

    return x, pd.DataFrame(recorder, columns=['STEP', 'X','gradient', 'F(X)'])

# 求解
x_opt_gd,recorder_gd = gradient_descent(f,df_dx, x0)
print("Gradient Descent:")
print("Optimal Point:", x_opt_gd)
print("Minimum Value of f:", f(x_opt_gd))

recorder_gd
```

![](https://cdn.jsdelivr.net/gh/oixel64/imgs/imgs/202405031802000.png)


![](https://cdn.jsdelivr.net/gh/oixel64/imgs/imgs/202405031803526.png)


# 2. 最速法（Steepest Descent）

## 2.1 原理

最速法其实与梯度法非常非常类似，唯一区别在于**最速法的步长$\alpha_k$是动态变化的，而非梯度法中的常数$\alpha$**。最速法中的每一次迭代需要通过**一维搜索**算法重新计算步长。

**本质：**

- 最速法是梯度法的一种实现

- 是用**线性函数**去近似目标函数
  
  最速下降法可以被解释为在每一步迭代中使用当前点处的线性函数（梯度）来近似目标函数，并沿着该线性函数的反方向进行搜索

## 2.2 算法步骤

1. **初始化**：

   - 选择初始点 $x_0$
   - 设定停止条件 $\epsilon$
   - 最大迭代次数

2. **计算梯度**：
   $$
   g_t = \nabla f(x_t)
   $$

3. **更新参数**：

   $$
   {x}_{t+1} = {x}_t - \alpha_k g_t
   $$
    **其中，$\alpha_k$ 是学习率（步长），利用一维搜索确定**

4. **收敛判断**：
   - 若 $\| g_k \|$ 小于预先设定的阈值 $\epsilon$，则停止迭代
   - 若 达到最大迭代次数，则也停止迭代

5. **迭代**：
   重复2-4

-------------------

**一维搜索算法**：

1. **精确线搜索**
   
    **Armijo条件：**

    $$ f(x_k + \alpha_k p_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T p_k $$

    其中：
    - $f(x_k)$ 是当前点 $x_k$ 的目标函数值，
    - $p_k$ 是搜索方向，
    - $\alpha_k$ 是步长，
    - $c_1$ 是一个控制参数，通常取值范围为 $(0,1)$。


2. **非精确线搜索**
    
    **强Wolfe条件：**

    $$ f(x_k + \alpha_k p_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T p_k $$
    $$ | \nabla f(x_k + \alpha_k p_k)^T p_k | \leq c_2 | \nabla f(x_k)^T p_k | $$

    其中：
    - $f(x_k)$ 是当前点 $x_k$ 的目标函数值，
    - $p_k$ 是搜索方向，
    - $\alpha_k$ 是步长，
    - $c_1$ 和 $c_2$ 是两个控制参数，通常取值范围为 $0 < c_1 < c_2 < 1$。

    **弱Wolfe条件（一般使用）：**

    $$ | \nabla f(x_k + \alpha_k p_k)^T p_k | \leq c_2 | \nabla f(x_k)^T p_k | $$

3. **一维搜索框架**

   - 输入：目标函数 $f(x)$，搜索方向 $p$，当前点 $x$。
   - 输出：最优步长 $\alpha$。
   - 步骤：
      1. 选择一个合适的初始步长 $\alpha_0$。
      2. 迭代调整步长，直到满足某个终止条件，比如 Armijo 条件。
      3. 返回满足条件的步长 $\alpha$。

## 3.3 代码实现

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.rc('text', usetex=True)

# 定义函数
def f(x):
    x_1, x_2, x_3 = x
    return (x_1 - 4)**4 + (x_2 - 3)**2 + 4*(x_3 + 5)**4

# 定义函数导数
def df_dx(x):
    x_1, x_2, x_3 = x
    df_dx1 = 4 * (x_1 - 4)**3
    df_dx2 = 2 * (x_2 - 3)
    df_dx3 = 16 * (x_3 + 5)**3
    return np.array([df_dx1, df_dx2, df_dx3])

# 初始点
x0 = np.array([4.0, 2.0, -1.0])

# Armijo条件线搜索，用来找到一个合适的步长
def line_search(f, df, x, d):
    alpha = 0.1
    beta = 0.5
    t = 1
    while f(x + t*d) > f(x) + alpha * t * np.dot(df(x), d):
        t = beta * t
    return t

# 最速下降法
def steepest_descent(f, df_dx, x0, epsilon=1e-6, max_iters=1000):
    x = x0
    recorder = list()
    recorder.append([0,x,0,0,f(x)])

    for i in range(max_iters):
        grad = df_dx(x)
        if np.linalg.norm(grad) < epsilon:
            break
        direction = -grad
        step_size = line_search(f, df_dx, x, direction)
        x = x + step_size * direction
        recorder.append([i+1,x,direction,step_size,f(x)])
    else:
        Warning("达到最大迭代次数，未能收敛")
    return x, pd.DataFrame(recorder, columns=['STEP', 'X', 'direction','step_size','F(X)'])

x_opt_sd, recorder_sd = steepest_descent(f, df_dx, x0)
print("Steepest Descent:")
print("Optimal solution:", x_opt_sd)
print("Optimal value of f(x):", f(x_opt_sd))

# 迭代过程
show_path(recorder_sd, "Steepest Descent Path")
```

![](https://cdn.jsdelivr.net/gh/oixel64/imgs/imgs/202405031852050.png)

![](https://cdn.jsdelivr.net/gh/oixel64/imgs/imgs/202405031852401.png)


# 3.牛顿法（Newton Method）

## 3.1 原理

由于最速下降法速度慢，**Newton引入二阶梯度加速**，通过求其Hesse矩阵，一步到位直接求到极小点

牛顿法的原理是基于使用一个**二次函数**去近似目标函数，然后精确地求出这个二次函数的极小点作为新的迭代点，从而逐步优化目标函数的过程

1. **二次函数的选取**：牛顿法通过在当前点处使用目标函数的二阶泰勒展开式来构建一个二次函数作为近似。在点 $x_k$ 处，目标函数 $f(x)$ 的二阶泰勒展开式为：

   $$ f(x+s) \approx f(x_k) + \nabla f(x_k)^{T}s + \frac{1}{2} s^{T} \nabla^2 f(x_k) s =: m_k(s) $$

   其中，$\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度，$\nabla^2 f(x_k)$ 是目标函数在 $x_k$ 处的Hessian矩阵。

2. **迭代方向的选取**：牛顿法的关键在于求解二次函数 $m_k(s)$ 的极小点，作为下一步迭代的方向。通过令二次函数的一阶导数为零，即 $\nabla m_k(s) = 0$，解出极小点所对应的 $s$ 值，即为牛顿方向。

3. **迭代更新**：一旦求得牛顿方向 $s$，将其应用于当前点 $x_k$，即 $x_{k+1} = x_k + s$，得到下一次迭代的点 $x_{k+1}$。

4. **收敛性**：如果初始点选择得当，且目标函数在局部区域内表现得足够光滑，那么牛顿法往往能够快速收敛到局部极小值点。这是因为牛顿法具有至少二阶收敛速度，即每次迭代都会更快地逼近极小值点。

总结，牛顿法利用二次函数近似目标函数，并通过精确求解二次函数的极小点来指导迭代方向，以加速目标函数的优化过程。


## 3.2 算法步骤

1. **初始化**：
   - 选择初始点 $x_0$。
   - 设置精度参数 $\epsilon$，用于控制算法收敛的条件。
   - 初始化迭代次数 $k = 0$。


2. **迭代过程**：

   - **计算梯度和Hessian矩阵**：
     - 计算目标函数在当前点 $x_k$ 处的梯度 $g_k = \nabla f(x_k)$。
     - 计算目标函数在当前点 $x_k$ 处的Hessian矩阵 $H_k = \nabla^2 f(x_k)$。
  
   - **解牛顿方程**：
     - 计算牛顿方向 $d_k^N = - (H_k)^{-1} g_k$。
   
   - **更新参数**：
     - 更新迭代点：$x_{k+1} = x_k + d_k^N$。


3. **判断停止条件**：
   - 如果 $\| g_k \| \leq \epsilon$，则算法收敛，停止迭代。
   - 否则，将 $k$ 更新为 $k + 1$，并回到步骤2。

## 3.3 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math

%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.rc('text', usetex=True)
plt.rc('font', family='serif')

# 定义函数
def f(x):
    return x**2 + 4 * np.cos(x)

# 一阶函数
def df(x):
    return 2*x - 4 * np.sin(x)

# 二阶函数
def ddf(x):
    return 2 - 4 * np.cos(x)

def newton_method(f, df, ddf, x0, tol=1e-5, max_iter=1000):
    x = x0
    iteration = 0
    while abs(df(x)) > tol and iteration < max_iter:
        x = x - df(x) / ddf(x)
        iteration += 1
    return x

x0 = 1
min_x_newton = newton_method(f, df, ddf, x0)
print("最小值的近似解为:", min_x_newton)
print("最小值点的函数值为:", f(min_x_newton))
```



